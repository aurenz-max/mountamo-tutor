# Visual Schema Recommender - LLM Implementation Summary

## Overview

Successfully refactored the Visual Schema Recommender from a hardcoded rule-based system to an intelligent LLM-powered recommendation engine that leverages the full catalog of 26 visual schemas with their metadata from BigQuery.

## What Changed

### Files Modified

1. **[app/core/database.py](curriculum-authoring-service/app/core/database.py#L235-L261)**
   - Added `get_all_visual_schemas_with_metadata()` method
   - Queries `curriculum_primitives` table for all schemas with metadata
   - Returns: primitive_id, primitive_name, category, best_for, avoid_for, example

2. **[app/core/config.py](curriculum-authoring-service/app/core/config.py#L44-L49)**
   - Added 5 new configuration settings:
     - `SCHEMA_RECOMMENDER_USE_LLM` - Enable/disable LLM (default: True)
     - `SCHEMA_RECOMMENDER_MODEL` - Gemini model (default: "gemini-1.5-flash")
     - `SCHEMA_RECOMMENDER_TEMPERATURE` - Consistency setting (default: 0.3)
     - `SCHEMA_RECOMMENDER_MAX_SCHEMAS` - Max recommendations (default: 7)
     - `SCHEMA_RECOMMENDER_CACHE_TTL_MINUTES` - Cache duration (default: 60)

3. **[app/generators/visual_schema_recommender.py](curriculum-authoring-service/app/generators/visual_schema_recommender.py)**
   - **Complete rewrite** (139 lines â†’ 429 lines)
   - Added LLM-powered recommendation with Gemini API
   - Implemented multi-level caching (schema metadata + recommendations)
   - Preserved legacy rule-based system as fallback
   - New methods:
     - `recommend_schemas_with_llm()` - Main LLM recommendation method
     - `_get_schemas_with_metadata()` - Fetch and cache schema catalog
     - `_build_schema_catalog_prompt()` - Format schemas for LLM
     - `_build_llm_prompt()` - Build complete prompt with context
     - `recommend_schemas_legacy()` - Preserved original logic

4. **[app/services/foundations_service.py](curriculum-authoring-service/app/services/foundations_service.py#L90-L101)**
   - Updated to call `recommend_schemas_with_llm()` asynchronously
   - Passes master_context and context_primitives to recommender
   - Converts Pydantic models to dicts for LLM processing

### Files Created

5. **[tests/test_visual_schema_recommender.py](curriculum-authoring-service/tests/test_visual_schema_recommender.py)**
   - Comprehensive unit tests (400+ lines)
   - Test classes:
     - `TestLegacyRecommender` - Rule-based system tests
     - `TestLLMRecommender` - LLM system tests with mocking
     - `TestPromptBuilding` - Prompt generation tests
     - `TestUtilityMethods` - Helper method tests
     - `TestRecommenderIntegration` - Integration tests
   - Includes fixtures for various subjects and grade levels

6. **[tests/manual_test_recommender.py](curriculum-authoring-service/tests/manual_test_recommender.py)**
   - Manual testing script with 5 test cases:
     - Kindergarten Math (Counting)
     - 3rd Grade Science (Life Cycles)
     - Kindergarten Literacy (Letters)
     - 2nd Grade Math (Addition)
     - 4th Grade Language Arts (Story Structure)
   - Compares LLM vs legacy recommendations
   - Can be run directly: `python -m tests.manual_test_recommender`

7. **[docs/visual_schema_recommender.md](curriculum-authoring-service/docs/visual_schema_recommender.md)**
   - Complete documentation (400+ lines)
   - Architecture overview
   - Usage examples
   - Configuration guide
   - Testing instructions
   - Performance considerations
   - Troubleshooting guide

## Key Improvements

### Before (Rule-Based System)
```python
# Hardcoded keyword mappings
keyword_schema_map = {
    'count': ['object-collection', 'number-line'],
    'fraction': ['fraction-circles'],
    'cycle': ['cycle-diagram'],
    # ... 14 total patterns
}
```

### After (LLM-Powered System)
```python
# LLM receives full catalog with metadata
schemas = await recommender.recommend_schemas_with_llm(
    subskill_data,
    master_context={
        'core_concepts': [...],
        'learning_objectives': [...]
    },
    context_primitives={
        'scenarios': [...],
        'concrete_objects': [...]
    }
)
# LLM returns: ["object-collection", "number-line", ...]
# Plus reasoning: "These schemas support hands-on counting..."
```

## Benefits

### 1. Semantic Understanding
- LLM comprehends nuanced relationships between subskills and visual types
- Considers grade-level appropriateness and cognitive development
- Understands context beyond simple keyword matching

### 2. Metadata-Driven
- Leverages existing `best_for` and `avoid_for` guidance from database
- Uses concrete examples to inform recommendations
- No hardcoded mappings to maintain

### 3. Context-Aware
- Considers master context (core concepts, learning objectives)
- Uses context primitives (scenarios, objects) for richer understanding
- Adapts to the full learning context, not just the subskill text

### 4. Maintainable
- New schemas automatically discovered from database
- No code changes needed when adding visual primitives
- Centralized schema metadata in BigQuery

### 5. Explainable
- LLM provides reasoning for each recommendation set
- Logs show decision-making process
- Educators can understand "why" these schemas were chosen

### 6. Robust
- Automatic fallback to legacy system on errors
- Multi-level caching for performance
- Validates all schema IDs before returning

## How It Works

### LLM Prompt Structure

The LLM receives a comprehensive prompt:

```markdown
You are an educational technology expert...

## Subskill Details
- Subject: Mathematics
- Grade Level: Kindergarten
- Skill: Counting and Number Recognition
- Subskill: Count objects from 1 to 10

### Core Concepts
- One-to-one correspondence
- Number sequence
- Quantity recognition

### Learning Objectives
- Count objects accurately from 1 to 10
- Match numbers to quantities

### Example Learning Scenarios
- Counting toys in a classroom
- Counting apples at a fruit stand

# Available Visual Schemas

## FOUNDATIONAL Category

### object-collection
**Name:** Object Collection
**Best For:** Counting discrete objects, showing groups of items
**Avoid For:** Abstract numerical data, complex relationships
**Example:** Show 5 purple balls, display 3 apples and 2 bananas

[... 25 more schemas with metadata ...]

## Your Task
Recommend 7 visual schemas that would be most effective for teaching this subskill.

## Selection Criteria
1. Age-appropriateness - Match visual complexity to grade level
2. Conceptual alignment - Choose schemas that naturally represent concepts
3. Learning objectives - Prioritize schemas that achieve stated goals
4. Best-for guidance - Respect metadata for each schema
5. Variety - Include diverse representation types

## Output Format
{
  "recommended_schemas": ["schema-id-1", "schema-id-2", ...],
  "reasoning": "Brief explanation of why these schemas work together"
}
```

### Response Processing

1. **Parse JSON** - Extract recommended_schemas and reasoning
2. **Validate** - Check each schema ID exists in database
3. **Filter** - Remove any invalid IDs
4. **Log** - Record recommendations and reasoning
5. **Cache** - Store for 60 minutes (per subskill)
6. **Return** - List of validated schema IDs

### Fallback Strategy

```
LLM Recommendation
  â”œâ”€ Success â†’ Return LLM recommendations
  â”œâ”€ API Error â†’ Fall back to legacy system
  â”œâ”€ Invalid JSON â†’ Fall back to legacy system
  â”œâ”€ No valid schemas â†’ Fall back to legacy system
  â””â”€ Client not initialized â†’ Fall back to legacy system

Legacy System (Rule-Based)
  â”œâ”€ Grade-based rules (K-1 gets foundational)
  â”œâ”€ Subject-based mapping (math â†’ math schemas)
  â”œâ”€ Keyword matching (14 patterns)
  â””â”€ Default fallback (versatile schemas)
```

## Testing

### Run Unit Tests
```bash
cd curriculum-authoring-service
pytest tests/test_visual_schema_recommender.py -v
```

**Test Coverage:**
- âœ… Legacy rule-based recommendations
- âœ… LLM recommendations with mocking
- âœ… Caching behavior (schema metadata + recommendations)
- âœ… Prompt building with/without context
- âœ… Error handling and fallbacks
- âœ… Schema ID validation
- âœ… Utility methods

### Run Manual Tests
```bash
cd curriculum-authoring-service
python -m tests.manual_test_recommender
```

**Output:**
```
Configuration:
  LLM Enabled: True
  Model: gemini-1.5-flash
  Temperature: 0.3
  Max Schemas: 7

Test Case 1: Kindergarten Math - Counting
Subject: Mathematics
Grade: Kindergarten
Skill: Counting and Number Recognition
Subskill: Count objects from 1 to 10

ðŸ¤– Testing LLM-based recommendations...
âœ… LLM Recommended 5 schemas:
   - object-collection
   - comparison-panel
   - number-line
   - bar-model
   - base-ten-blocks

ðŸ”§ Testing legacy rule-based recommendations...
âœ… Legacy Recommended 7 schemas:
   - object-collection
   - comparison-panel
   - bar-model
   - number-line
   - base-ten-blocks
   - fraction-circles
   - geometric-shape

ðŸ“Š Overlap: 5 schemas in common
```

## Configuration

### Environment Variables

Add to `.env`:

```bash
# LLM Configuration for Visual Schema Recommender
SCHEMA_RECOMMENDER_USE_LLM=True
SCHEMA_RECOMMENDER_MODEL=gemini-flash-lite-latest
SCHEMA_RECOMMENDER_TEMPERATURE=0.3
SCHEMA_RECOMMENDER_MAX_SCHEMAS=7
SCHEMA_RECOMMENDER_CACHE_TTL_MINUTES=60
```

### Disable LLM (Cost Savings)

```bash
# Use legacy rule-based system
SCHEMA_RECOMMENDER_USE_LLM=False
```

## Performance

### API Costs (per recommendation)
- **Model:** gemini-1.5-flash (most cost-effective)
- **Input tokens:** ~1500-2500 (schema catalog + context)
- **Output tokens:** ~100-150 (recommendations + reasoning)
- **Cost:** < $0.001 per recommendation
- **Caching:** 60-minute cache reduces repeated API calls

### Latency
- **First request:** 2-4 seconds (DB query + LLM)
- **Cached request:** < 100ms (memory)
- **Legacy fallback:** < 50ms (rule-based)

### Optimization
- Schema metadata cached for 30 minutes
- Recommendations cached for 60 minutes per subskill
- BigQuery result caching (automatic)
- Low temperature (0.3) for consistency

## Migration Path

### No Breaking Changes
Existing code continues to work:

```python
# Old synchronous call (uses legacy system)
schemas = recommender.recommend_schemas(subskill_data)
```

### Opt-in to LLM
Update to async for LLM benefits:

```python
# New async call (uses LLM with full context)
schemas = await recommender.recommend_schemas_with_llm(
    subskill_data,
    master_context=context,
    context_primitives=primitives
)
```

### Gradual Rollout
1. **Phase 1:** Enable LLM in dev/staging (`SCHEMA_RECOMMENDER_USE_LLM=True`)
2. **Phase 2:** Monitor logs and compare with legacy recommendations
3. **Phase 3:** Gather educator feedback on recommendation quality
4. **Phase 4:** Enable in production with monitoring
5. **Phase 5:** Deprecate legacy system (if desired)

## Monitoring

### Key Metrics to Track
- **Cache hit rate** - How often recommendations are cached
- **LLM API latency** - Response time from Gemini
- **Fallback frequency** - How often legacy system is used
- **Schema diversity** - Variety in recommendations
- **Educator edits** - How often recommendations are modified

### Log Patterns

**Success:**
```
ðŸ¤– Requesting LLM recommendations for Count objects from 1 to 10...
âœ… LLM recommended 5 schemas: object-collection, number-line, comparison-panel
ðŸ’­ Reasoning: These schemas support hands-on counting and visual comparison...
```

**Cache Hit:**
```
ðŸ“¦ Using cached recommendations for Mathematics:Count objects from 1 to 10
```

**Fallback:**
```
âš ï¸ Gemini client not available, falling back to rule-based system
ðŸ”§ Using legacy rule-based recommendation system
âœ… Legacy system recommended 7 schemas: object-collection, comparison-panel...
```

## Next Steps

### Immediate Actions
1. âœ… Code implementation complete
2. âœ… Unit tests written
3. âœ… Manual tests created
4. âœ… Documentation complete
5. â³ Deploy to staging environment
6. â³ Run integration tests with real data
7. â³ Gather educator feedback
8. â³ Monitor performance metrics

### Future Enhancements
- **Personalization** - Adapt to student learning styles
- **A/B Testing** - Compare LLM vs legacy effectiveness
- **Analytics** - Track which schemas improve learning outcomes
- **Feedback Loop** - Learn from educator edits
- **Multi-language** - Localized recommendations
- **Schema Combinations** - Suggest complementary pairs

## Rollback Plan

If issues arise:

1. **Immediate:** Set `SCHEMA_RECOMMENDER_USE_LLM=False` in environment
2. **Service restart** not required - config change takes effect immediately
3. **Legacy system** continues to work as before
4. **No data loss** - recommendations are generated on-demand
5. **Investigate** logs to identify root cause
6. **Fix and redeploy** when ready

## Success Criteria

âœ… **Implementation Complete:**
- All files modified/created
- Unit tests passing
- Manual tests working
- Documentation complete

âœ… **Backward Compatible:**
- Existing API unchanged
- Legacy system preserved
- No breaking changes

âœ… **Robust:**
- Automatic fallback on errors
- Multi-level caching
- Comprehensive error logging

âœ… **Maintainable:**
- Well-documented code
- Comprehensive tests
- Clear configuration

## Summary

Successfully transformed the Visual Schema Recommender from a rigid rule-based system into an intelligent, context-aware LLM-powered recommendation engine. The new system:

- âœ… Provides semantic understanding of educational needs
- âœ… Leverages full schema catalog with metadata
- âœ… Considers learning context (concepts, objectives, scenarios)
- âœ… Maintains backward compatibility
- âœ… Includes robust fallback mechanisms
- âœ… Optimized for performance with multi-level caching
- âœ… Fully tested with comprehensive test suite
- âœ… Well-documented for future maintenance

The implementation is production-ready and can be enabled by setting `SCHEMA_RECOMMENDER_USE_LLM=True` in the environment configuration.
