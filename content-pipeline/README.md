# Content Evaluation Framework V2 - Schema-Driven Validation

A robust, declarative content quality evaluation system for educational content generation.

## Overview

This framework provides a **schema-driven, three-tier evaluation pipeline** for content generated by your `/generate` endpoint:

1. **Tier 1: Structural Validation (Schema-Driven)** - Dynamic validation using YAML schema registry
2. **Tier 2: Heuristic Validation** - Readability, visual coherence, UI overflow detection
3. **Tier 3: LLM-as-Judge** - Pedagogical quality, correctness, clarity, bias detection

### V2 Architecture Highlights

- **Declarative, Not Imperative** - Validation rules defined in YAML, not hardcoded Python
- **Zero-Code Problem Type Addition** - Add new types by creating a YAML file
- **Robust Null Handling** - Optional fields can be `null` without failing validation
- **Eliminates Brittleness** - No code changes needed when backend schemas evolve

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    Content Pipeline Tool                     │
│                      (content_ops.py)                        │
└──────────────┬──────────────────────────────────┬───────────┘
               │                                   │
        ┌──────▼──────┐                    ┌──────▼──────┐
        │   Backend   │                    │   Gemini    │
        │  API Client │                    │  AI Judge   │
        └──────┬──────┘                    └─────────────┘
               │
        ┌──────▼────────────────────────┐
        │  Your FastAPI Backend         │
        │  - /curriculum/{subject}      │
        │  - /problems/generate         │
        └───────────────────────────────┘
```

## Quick Start

### 1. Installation

```bash
cd content-pipeline
pip install -r requirements.txt
```

### 2. Configuration

Copy the example environment file and configure:

```bash
cp .env.example .env
```

Edit `.env`:

```env
# Backend Configuration
BACKEND_URL=http://localhost:8000
AUTH_TOKEN=your_auth_token_here  # Optional, if your backend requires auth

# Gemini API Configuration
GEMINI_API_KEY=your_gemini_api_key_here

# Evaluation Settings
DEFAULT_MODEL=flash  # Options: flash, flash-lite
```

### 3. Start Your Backend

Ensure your FastAPI backend is running locally:

```bash
cd ../backend
uvicorn app.main:app --reload
```

### 4. Run Your First Evaluation

```bash
# Check backend connectivity
python content_ops.py health-check

# List available curriculum
python content_ops.py list-curriculum --subject math

# Test generation for a specific skill (10 problems)
python content_ops.py test-generation --subject math --skill-id "counting" --max-tests 10

# Full evaluation with LLM judge
python content_ops.py test-generation --subject math --max-tests 20 --model flash
```

## Usage Examples

### Basic Testing

```bash
# Test 10 kindergarten math problems
python content_ops.py test-generation --subject math --grade K --max-tests 10

# Test a specific subskill
python content_ops.py test-generation --subject math --subskill-id "count-to-5" --max-tests 5

# Quick structural + heuristic check (skip expensive LLM calls)
python content_ops.py test-generation --subject reading --max-tests 20 --skip-llm
```

### Model Selection

```bash
# High-quality evaluation (Gemini 1.5 Flash)
python content_ops.py test-generation --subject math --max-tests 10 --model flash

# Fast, cost-effective evaluation (Gemini Flash Lite)
python content_ops.py test-generation --subject math --max-tests 50 --model flash-lite

# Heuristic-only (no LLM cost)
python content_ops.py test-generation --subject math --max-tests 100 --skip-llm
```

### Large-Scale Audits

```bash
# Full curriculum audit (WARNING: This will be expensive and slow)
python content_ops.py test-generation --all-subjects --max-tests 1000 --model flash-lite
```

## Understanding the Evaluation Reports

### Console Output

After running an evaluation, you'll see a summary table:

```
                    Evaluation Summary
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃ Metric                      ┃            Count ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ Total Problems Evaluated    │               20 │
│ Passed Structural Validation│       18 (90.0%) │
│ Passed Heuristic Validation │       16 (80.0%) │
│                             │                  │
│ ✅ Approved                 │       14 (70.0%) │
│ ⚠️  Needs Revision          │        4 (20.0%) │
│ ❌ Rejected                 │        2 (10.0%) │
│                             │                  │
│ Avg Pedagogy Score          │         8.25/10  │
│ Avg Clarity Score           │         7.90/10  │
│ Avg Correctness Score       │         9.10/10  │
└─────────────────────────────┴──────────────────┘
```

### CSV Report

Reports are saved to `reports/eval-{timestamp}.csv` with these columns:

#### Curriculum Context
- `subject`, `skill_id`, `subskill_id`, `grade_level`

#### Generation Metadata
- `problem_id`, `problem_type`, `generation_successful`, `generation_time_ms`

#### Tier 1: Structural Validation
- `tier1_pass` - Boolean, did it pass schema validation?
- `tier1_issues` - Semicolon-separated list of structural issues

#### Tier 2: Heuristic Validation
- `tier2_pass` - Boolean
- `readability_score` - Flesch-Kincaid grade level
- `total_char_count` - Total characters in problem
- `visual_coherence_pass` - Boolean, no UI overflow risk?
- `visual_overflow_risk` - Boolean, text might overflow UI?
- `tier2_issues` - Heuristic failures

#### Tier 3: LLM Judgment
- `pedagogy_score` (1-10) - Pedagogical alignment
- `clarity_score` (1-10) - Clarity and age-appropriateness
- `correctness_score` (1-10) - Answer correctness
- `bias_score` (1-10) - Bias/inclusivity (10 = unbiased)
- `overall_quality` - excellent | good | needs_revision | unacceptable
- `recommended_action` - approve | minor_edits | major_revision | reject

#### Final Assessment
- `final_recommendation` - approve | revise | reject
- `overall_score` - Composite score (0-10)

## Visual Coherence Checks

**This addresses your concern about text rendering in the UI.**

The Tier 2 heuristic validator includes these critical checks:

### Character Limits (UI Overflow Prevention)
- **Questions/Statements:** ≤ 500 characters
- **Multiple choice options:** ≤ 100 characters each
- **Teaching notes:** ≤ 800 characters

### Word Length (Prevents Horizontal Overflow)
- No single word exceeds 25 characters
- Catches problematic words that break layout

### Line Breaks
- No field exceeds 10 line breaks
- Prevents excessive vertical spacing

### Forbidden Content
- Detects raw HTML tags (e.g., `<h1>`, `<script>`)
- Flags unescaped special characters
- Prevents rendering issues

**Example Failure:**
```
Problem rejected: visual_coherence_pass = false
Issues:
  - question exceeds character limit: 612 > 500
  - options[2] contains word too long (32 chars): 'antidisestablishmentarianism'
  - teaching_note has too many line breaks: 15 > 10
```

## Cost Estimation

### Gemini Pricing (as of 2024)
- **Flash Lite:** ~$0.0015 per 1000 problems (batch mode)
- **Flash:** ~$0.003 per 1000 problems (batch mode)
- **Flash (real-time):** ~$0.006 per 1000 problems

### Recommendation
- **CI/CD Integration:** Use Flash Lite + `--skip-llm` for fast feedback
- **Weekly Audits:** Use Flash Lite in batch mode
- **Critical Content:** Use Flash for high-stakes assessments
- **Debugging:** Use `--skip-llm` for instant structural/heuristic feedback

## Integration with Your Codebase

This tool directly integrates with your backend endpoints:

### Backend Endpoints Used
- **`GET /api/curriculum/subjects`** - List all subjects
- **`GET /api/curriculum/curriculum/{subject}`** - Get curriculum structure
- **`POST /api/problems/generate`** - Generate problems for evaluation

### Schema-Driven Validation (V2)

The structural validator uses a **schema registry** loaded from `schemas/` directory:
- **Backend Source:** `backend/app/generators/content_schemas.py` (PRACTICE_PROBLEMS_SCHEMA_STEP1)
- **Schema Files:** 8 YAML files defining validation rules for each problem type
- **Visual Support:** Handles both `*_visual_intent` (Step 1) and `*_visual_data` (Step 2) outputs
- **Null Handling:** Optional fields can be `null` without causing validation failures

#### Supported Problem Types

1. **multiple_choice** - MCQ with options ([schemas/multiple_choice.yaml](schemas/multiple_choice.yaml))
2. **true_false** - True/false statements ([schemas/true_false.yaml](schemas/true_false.yaml))
3. **fill_in_blanks** - Fill-in-the-blank problems ([schemas/fill_in_blanks.yaml](schemas/fill_in_blanks.yaml))
4. **matching_activity** - Match left to right items ([schemas/matching_activity.yaml](schemas/matching_activity.yaml))
5. **sequencing_activity** - Order items correctly ([schemas/sequencing_activity.yaml](schemas/sequencing_activity.yaml))
6. **categorization_activity** - Sort into categories ([schemas/categorization_activity.yaml](schemas/categorization_activity.yaml))
7. **scenario_question** - Scenario-based questions ([schemas/scenario_question.yaml](schemas/scenario_question.yaml))
8. **short_answer** - Open-ended responses ([schemas/short_answer.yaml](schemas/short_answer.yaml))

## Adding a New Problem Type (V2)

**No Python code changes required!** Just create a schema file:

### Step 1: Create Schema YAML File

Create `schemas/your_problem_type.yaml`:

```yaml
type_name: your_problem_type

required_fields:
  - id
  - difficulty
  - grade_level
  - your_custom_field

optional_fields:
  - optional_visual_field

field_types:
  id: string
  difficulty: enum
  grade_level: string
  your_custom_field: string
  optional_visual_field: dict_or_null

enum_values:
  difficulty:
    - easy
    - medium
    - hard

# For nested structures (like options in MCQ)
nested_structures:
  your_list_field:
    required_keys:
      - id
      - text
    key_types:
      id: string
      text: string

# If problem supports visuals
visual_field: your_visual_intent
visual_field_alt: your_visual_data
```

### Step 2: Test It

```bash
# The validator automatically picks up the new schema
python content_ops.py test-generation --subject your-subject --max-tests 5
```

That's it! The framework now validates your new problem type.

### Schema Field Types

- `string` - String value
- `boolean` - Boolean (true/false)
- `integer` - Integer number
- `enum` - Enumerated value (checked against `enum_values`)
- `list_of_string` - Array of strings
- `list_of_dict` - Array of objects (validated against `nested_structures`)
- `dict` - Object/dictionary
- `dict_or_null` - Object or null (for optional fields)

## Troubleshooting

### Backend Connection Failed
```bash
# Check if backend is running
python content_ops.py health-check

# Try with explicit URL
python content_ops.py health-check --backend-url http://localhost:8000
```

### Authentication Errors
If your backend requires authentication, ensure `AUTH_TOKEN` is set in `.env`:
```env
AUTH_TOKEN=your_firebase_jwt_or_api_key
```

### Gemini API Errors
```bash
# Verify API key is set
echo $GEMINI_API_KEY  # Linux/Mac
echo %GEMINI_API_KEY%  # Windows

# Test with a small batch first
python content_ops.py test-generation --subject math --max-tests 2 --model flash
```

### Empty Results
If no curriculum nodes are found:
```bash
# List available curriculum to verify structure
python content_ops.py list-curriculum

# Check filtering
python content_ops.py test-generation --subject math --grade K --max-tests 5
```

### V2 Schema Issues

#### "No schema found for problem type 'X'"

**Cause:** Missing schema file or `type_name` mismatch

**Solution:**
1. Check that `schemas/X.yaml` exists
2. Verify the `type_name` field in the YAML matches the problem's `problem_type`
3. Check file permissions (YAML must be readable)

#### "visual_data must be a dict" for null visual fields

**Cause:** This should NOT happen in V2 - optional visual fields can be null

**Solution:**
1. Verify you're using the refactored `StructuralValidator` (check file date)
2. Ensure the field is listed in `optional_fields` in the schema
3. Check validator logs for null handling confirmation

#### False positive: "Problem contains placeholder text" for ellipsis (...)

**Cause:** Old version of `HeuristicValidator` (pre-V2)

**Solution:** The ellipsis pattern has been removed from `PLACEHOLDER_PATTERNS` in V2. Check your `heuristics_validator.py` file date.

#### Schema loading errors

```bash
# Enable debug logging to see schema loading
export LOG_LEVEL=DEBUG
python content_ops.py test-generation --subject math --max-tests 1

# You should see:
# INFO: Loaded 8 problem type schemas
# DEBUG: Loaded schema for 'multiple_choice' from multiple_choice.yaml
# ...
```

## Project Structure

```
content-pipeline/
├── schemas/                    # ⭐ Schema registry (V2)
│   ├── multiple_choice.yaml       # MCQ validation rules
│   ├── true_false.yaml            # True/false validation rules
│   ├── fill_in_blanks.yaml        # Fill-in-blanks rules
│   ├── matching_activity.yaml     # Matching rules
│   ├── sequencing_activity.yaml   # Sequencing rules
│   ├── categorization_activity.yaml # Categorization rules
│   ├── scenario_question.yaml     # Scenario rules
│   └── short_answer.yaml          # Short answer rules
├── evaluation/
│   ├── __init__.py
│   ├── rubrics.py              # Pydantic models for evaluation data
│   ├── structural_validator.py # ⭐ Tier 1: Schema-driven validation (V2)
│   ├── heuristics_validator.py # ⭐ Tier 2: Readability + visual coherence (V2)
│   └── llm_judge.py            # Tier 3: Gemini-based evaluation
├── content_ops.py              # Main CLI entry point
├── api_client.py               # Backend API communication
├── reports/                    # Generated CSV reports (gitignored)
├── requirements.txt
├── .env                        # Configuration (gitignored)
└── README.md                   # ⭐ Updated for V2
```

### V2 Changes Summary

**New in V2:**
- `schemas/` directory with 8 YAML schema files (declarative validation rules)
- Refactored `structural_validator.py` with schema registry loader
- Fixed null handling for optional fields
- Removed ellipsis false positive from `heuristics_validator.py`
- Updated README with schema-driven architecture documentation

**Key Benefits:**
- Adding new problem types now takes **< 1 minute** (just create YAML file)
- No more Tier 1 failures when backend adds new problem types
- Clear, maintainable validation rules visible in YAML files
- No false positives for valid pedagogical content (e.g., ellipsis)

## Next Steps

### Phase 2: Production Data Integration
Once you're comfortable with local evaluation, extend this to analyze real student performance:

1. **Instrument Backend:** Capture completion rates, retry attempts, time-on-task
2. **Data Pipeline:** Aggregate metrics into BigQuery
3. **Dashboard:** Visualize low-performing content
4. **Feedback Loop:** Auto-regenerate failing content

### Phase 3: CI/CD Integration
Add this to your GitHub Actions workflow:

```yaml
- name: Content Quality Check
  run: |
    cd content-pipeline
    python content_ops.py test-generation --subject math --max-tests 20 --skip-llm
```

### Phase 4: A/B Testing
Generate variations and measure effectiveness:

```bash
# Generate 5 variations of a problem
python content_ops.py generate-variations --subskill-id "count-to-5" --count 5
```

## Support

For issues or questions:
1. Check this README
2. Review the `content_evaluation_plan.md` architecture document
3. Examine the CSV reports for detailed diagnostics
4. Enable debug logging: `export LOG_LEVEL=DEBUG`

---

**Built for:** Machine Learning Engineering Team
**Purpose:** Systematic content quality evaluation before production deployment
**Architecture:** Standalone local tool, no production dependencies
